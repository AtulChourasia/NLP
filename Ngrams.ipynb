{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/atul/anaconda3/envs/nlp/lib/python3.7/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/atul/anaconda3/envs/nlp/lib/python3.7/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/atul/anaconda3/envs/nlp/lib/python3.7/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: joblib in /home/atul/anaconda3/envs/nlp/lib/python3.7/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/atul/anaconda3/envs/nlp/lib/python3.7/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: importlib-metadata in /home/atul/anaconda3/envs/nlp/lib/python3.7/site-packages (from click->nltk) (6.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/atul/anaconda3/envs/nlp/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/atul/anaconda3/envs/nlp/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format. To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format. To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. To collect this data, we took conversations that AI trainers had with the chatbot. We randomly selected a model-written message, sampled several alternative completions, and had AI trainers rank them. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review = review.lower()\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we trained this model using reinforcement learning from human feedback  rlhf   using the same methods as instructgpt  but with slight differences in the data collection setup ',\n",
       " 'we trained an initial model using supervised fine tuning  human ai trainers provided conversations in which they played both sides the user and an ai assistant ',\n",
       " 'we gave the trainers access to model written suggestions to help them compose their responses ',\n",
       " 'we mixed this new dialogue dataset with the instructgpt dataset  which we transformed into a dialogue format ',\n",
       " 'to create a reward model for reinforcement learning  we needed to collect comparison data  which consisted of two or more model responses ranked by quality ',\n",
       " 'to collect this data  we took conversations that ai trainers had with the chatbot ',\n",
       " 'we randomly selected a model written message  sampled several alternative completions  and had ai trainers rank them ',\n",
       " 'using these reward models  we can fine tune the model using proximal policy optimization ',\n",
       " 'we performed several iterations of this process ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "model\n",
      "use\n",
      "reinforc\n",
      "learn\n",
      "human\n",
      "feedback\n",
      "rlhf\n",
      "use\n",
      "method\n",
      "instructgpt\n",
      "slight\n",
      "differ\n",
      "data\n",
      "collect\n",
      "setup\n",
      "train\n",
      "initi\n",
      "model\n",
      "use\n",
      "supervis\n",
      "fine\n",
      "tune\n",
      "human\n",
      "ai\n",
      "trainer\n",
      "provid\n",
      "convers\n",
      "play\n",
      "side\n",
      "user\n",
      "ai\n",
      "assist\n",
      "gave\n",
      "trainer\n",
      "access\n",
      "model\n",
      "written\n",
      "suggest\n",
      "help\n",
      "compos\n",
      "respons\n",
      "mix\n",
      "new\n",
      "dialogu\n",
      "dataset\n",
      "instructgpt\n",
      "dataset\n",
      "transform\n",
      "dialogu\n",
      "format\n",
      "creat\n",
      "reward\n",
      "model\n",
      "reinforc\n",
      "learn\n",
      "need\n",
      "collect\n",
      "comparison\n",
      "data\n",
      "consist\n",
      "two\n",
      "model\n",
      "respons\n",
      "rank\n",
      "qualiti\n",
      "collect\n",
      "data\n",
      "took\n",
      "convers\n",
      "ai\n",
      "trainer\n",
      "chatbot\n",
      "randomli\n",
      "select\n",
      "model\n",
      "written\n",
      "messag\n",
      "sampl\n",
      "sever\n",
      "altern\n",
      "complet\n",
      "ai\n",
      "trainer\n",
      "rank\n",
      "use\n",
      "reward\n",
      "model\n",
      "fine\n",
      "tune\n",
      "model\n",
      "use\n",
      "proxim\n",
      "polici\n",
      "optim\n",
      "perform\n",
      "sever\n",
      "iter\n",
      "process\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "for i in corpus:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
